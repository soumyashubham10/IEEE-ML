{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyashubham10/IEEE-ML/blob/main/Soumya_Shubham_IEEE_ML_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NufQy451LN2"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets sentence-transformers faiss-cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Environment Setup and Library Installation\n"
      ],
      "metadata": {
        "id": "rkywaTziRMe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"ms_marco\",\n",
        "    \"v1.1\",\n",
        "    split=\"train[:1%]\"\n",
        ")\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "wsaP-d57v6Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads a small subset of the MS MARCO v1.1 dataset using the HuggingFace datasets library.\n",
        "Only 1% of the training split is loaded to ensure fast execution in Google Colab.\n",
        "\n",
        "Hugging Face is used because it provides easy, reliable models. It allows direct loading of datasets like MS MARCO without manual downloads.\n",
        "\n"
      ],
      "metadata": {
        "id": "5At94JXySEzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.column_names)\n"
      ],
      "metadata": {
        "id": "17XIByltwEZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command displays the available columns in the MS MARCO dataset.\n",
        "It helps to understand the dataset structure before processing"
      ],
      "metadata": {
        "id": "44AJqc3FSxw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  'passage_text': '...',\n",
        "  'is_selected': 1\n",
        "}\n"
      ],
      "metadata": {
        "id": "hzLfm8qzwX20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"ms_marco\",\n",
        "    \"v1.1\",\n",
        "    split=\"train[:10%]\"\n",
        ")\n",
        "\n",
        "print(\"Queries loaded:\", len(dataset))\n"
      ],
      "metadata": {
        "id": "2Y_oXIj7w-fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads a portion of the MS MARCO dataset using the Hugging Face load_dataset function. Only 10% of the training data is loaded, which helps reduce memory usage and makes experimentation faster. The len(dataset) statement then prints the total number of query samples that were successfully loaded."
      ],
      "metadata": {
        "id": "Mb0GctZAEQuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = []\n",
        "relevant_docs = []\n",
        "\n",
        "for item in dataset:\n",
        "    queries.append(item[\"query\"])\n",
        "\n",
        "    rel = set()\n",
        "    for text, sel in zip(item[\"passages\"][\"passage_text\"],\n",
        "                         item[\"passages\"][\"is_selected\"]):\n",
        "        if sel == 1:\n",
        "            rel.add(text)\n",
        "\n",
        "    relevant_docs.append(rel)\n",
        "\n",
        "print(\"Total queries:\", len(queries))\n"
      ],
      "metadata": {
        "id": "9XMgQMTPSZwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "L8nrIFMYyfuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step imports some standard libraries."
      ],
      "metadata": {
        "id": "YvMoPG0AEgXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "id": "D-uq6GNHGf3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is used to fix the randomness in a program.\n",
        "The same seed value (42) is given to random, NumPy, and PyTorch so that every time the code runs, it produces the same random results.\n",
        "This helps in reproducibility, meaning experiments and results can be repeated and verified easil"
      ],
      "metadata": {
        "id": "OGFnetxWG7Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "doc_ids = []\n",
        "\n",
        "for item in dataset:\n",
        "    for passage in item[\"passages\"][\"passage_text\"]:\n",
        "        texts.append(passage)\n",
        "        doc_ids.append(len(doc_ids))\n",
        "        if len(texts) >= 75000:\n",
        "            break\n",
        "    if len(texts) >= 75000:\n",
        "        break\n",
        "\n",
        "corpus_df = pd.DataFrame({\n",
        "    \"doc_id\": doc_ids,\n",
        "    \"text\": texts\n",
        "})\n",
        "\n",
        "print(\"Total passages extracted:\", len(corpus_df))\n",
        "corpus_df.head()\n"
      ],
      "metadata": {
        "id": "2Pv6bBrhyxh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell extracts individual passages from the MS MARCO dataset and builds a document corpus.Each query in MS MARCO contains multiple passages.Passages are extracted sequentially and stored in a list.\n",
        "A unique doc_id is assigned to each passage.\n",
        "This DataFrame serves as the database for the semantic search engine.\n"
      ],
      "metadata": {
        "id": "0PPggzqMXQTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    \"doc_id\": doc_ids,\n",
        "    \"text\": texts\n",
        "})\n"
      ],
      "metadata": {
        "id": "9tg8jmqWi0y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total passages available:\", len(df))\n"
      ],
      "metadata": {
        "id": "7PbYk3-aTNh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the target was to extract 75,000 passages, the dataset subset loaded contained only 67,656 passages in total.\n",
        "\n",
        "This occurs because:\n",
        " MS MARCO passages are distributed unevenly across queries.\n",
        " Some queries contain fewer passages than others.\n",
        "\n",
        "Since the task allows flexibility in dataset size the extracted corpus size is still sufficient and valid for building and demonstrating a semantic search engine.\n"
      ],
      "metadata": {
        "id": "Il3vhlKgUZ3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "_x0iKyMoyY4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line loads the pre-trained all-MiniLM-L6-v2 SentenceTransformer model, which converts text passages into 384-dimensional dense embeddings that capture semantic meaning, allowing us to compare queries and documents using vector similarity for efficient and accurate semantic search."
      ],
      "metadata": {
        "id": "4G4eOyVNWjT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "6wMmePq40aBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code checks whether a CUDA-enabled GPU is available in the current runtime.  \n",
        "The torch.cuda.is_available function returns True if PyTorch can access a GPU, allowing computations to be accelerated using CUDA otherwise, it returns False, meaning all operations will run on the CPU.\n"
      ],
      "metadata": {
        "id": "MefaKztUYS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode(\n",
        "corpus_df[\"text\"].tolist(),\n",
        "batch_size=64,\n",
        "show_progress_bar=True,\n",
        "convert_to_numpy=True\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Embedding shape:\", embeddings.shape)"
      ],
      "metadata": {
        "id": "iMMZC6_WzE0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code turns all texts into numerical embeddings and stores them as a NumPy array. batch_size speeds it up, show_progress_bar shows progress and embeddings.shape tells the number of texts and embedding size."
      ],
      "metadata": {
        "id": "tvUwXSWzZAdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "\n",
        "print(\"Total vectors indexed:\", index.ntotal)"
      ],
      "metadata": {
        "id": "JdCxm6Lm20Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It prepares your embeddings for fast similarity search."
      ],
      "metadata": {
        "id": "_S8HAhH8ZgXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(preds, gold, k=10):\n",
        "    hit = 0\n",
        "    for p, g in zip(preds, gold):\n",
        "        if len(set(p[:k]) & g) > 0:\n",
        "            hit += 1\n",
        "    return hit / len(preds)\n",
        "\n",
        "\n",
        "def mrr_at_k(preds, gold, k=10):\n",
        "    total = 0\n",
        "    for p, g in zip(preds, gold):\n",
        "        for rank, doc in enumerate(p[:k], start=1):\n",
        "            if doc in g:\n",
        "                total += 1 / rank\n",
        "                break\n",
        "    return total / len(preds)\n",
        "\n",
        "\n",
        "def evaluate_dense_retriever(k=10, limit=500):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    for q, g in zip(queries[:limit], relevant_docs[:limit]):\n",
        "        res = semantic_search(q, top_k=k)\n",
        "        predictions.append(res[\"text\"].tolist())\n",
        "        gold.append(g)\n",
        "\n",
        "    print(\"Recall@10:\", recall_at_k(predictions, gold, k))\n",
        "    print(\"MRR@10:\", mrr_at_k(predictions, gold, k))\n"
      ],
      "metadata": {
        "id": "oCfJGNOhHgyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code checks how well a semantic search model is working. It runs search for each query and compares the results with the correct documents. Recall@10 shows whether at least one correct result appears in the top 10 and MRR@10 shows how high the first correct result is ranked. Together, these scores tell us how accurate and effective the search system is."
      ],
      "metadata": {
        "id": "yT9cXIxWCMdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, top_k=5):\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    scores, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = corpus_df.iloc[indices[0]].copy()\n",
        "    results[\"score\"] = scores[0]\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "J3i8nXCa24uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a query, converts it into an embedding and normalizes it. It then searches the FAISS index for the top_k most similar texts, retrieves them from the DataFrame and adds their similarity scores, and returns the results."
      ],
      "metadata": {
        "id": "m4qIpcw4Z0pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"tell me about ieee\"\n",
        "results = semantic_search(query, top_k=5)\n",
        "results"
      ],
      "metadata": {
        "id": "a6sBvCqq3LwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output shows the result of a semantic search performed with a query \"tell me about ieee\".  \n",
        "The table lists the top 5 most semantically similar passages, where doc_id identifies the document, text contains the retrieved passage.\n",
        "Higher scores indicate stronger semantic relevance, which is why the top results accurately describe IEEE.\n"
      ],
      "metadata": {
        "id": "-wdqBMZqIxJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(\"machine learning in healthcare\")\n"
      ],
      "metadata": {
        "id": "br0GIO2V5bH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line performs a semantic search for the query machine learning in healthcare."
      ],
      "metadata": {
        "id": "w5p7kVp6Jiog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about Electrical engineering\"\n",
        "results = semantic_search(query, top_k=10)\n",
        "\n",
        "for i, row in results.iterrows():\n",
        "    print(f\"\\nScore: {row['score']:.4f}\")\n",
        "    print(row['text'][:300], \"...\")\n"
      ],
      "metadata": {
        "id": "I0M87V2i5j5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version of the code differs in output format because the results are printed manually using a loop instead of being displayed as a DataFrame.  \n",
        "Rather than showing all columns  in tabular form, it prints each result one by one with its similarity score and only the first 300 characters of the passage.  \n",
        "This makes the output more readable and suitable for quick inspection.\n"
      ],
      "metadata": {
        "id": "N4ThOf-oKYkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_search(\"tell me about the first prime minister of india\", top_k=10)"
      ],
      "metadata": {
        "id": "QiJ49DlU5xo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rank-bm25\n"
      ],
      "metadata": {
        "id": "hLH1rDagTWKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs the rank-bm25 library which is required for the bonus part of the evaluation criteria."
      ],
      "metadata": {
        "id": "ZPos85qHU_bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Dense Retriever Evaluation ---\")\n",
        "evaluate_dense_retriever()\n"
      ],
      "metadata": {
        "id": "-kzVirNmRoP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function evaluate_dense_retriever() is then called to measure the performance of a dense retrieval model, such as how well it finds relevant documents for given queries."
      ],
      "metadata": {
        "id": "7r_TIcdR_ZCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n"
      ],
      "metadata": {
        "id": "5YqjqIqMTA74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_search(query, top_k=10):\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
        "    return corpus_df.iloc[top_idx]\n"
      ],
      "metadata": {
        "id": "QkL_X2CjThJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up BM25 document ranking for text search.\n",
        "Each document is tokenized by splitting text into words.\n",
        "The bm25_search function then takes a query, calculates relevance scores for all documents, sorts them in descending order and returns the top-k most relevant documents from the corpus."
      ],
      "metadata": {
        "id": "UXJnHW-SVXCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_search(\"machine learning in healthcare\")"
      ],
      "metadata": {
        "id": "8ZE05qVUTmNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic search looks at the meaning of the query. It can find related results even if the exact words are different.\n",
        "\n",
        "BM25 search looks for exact keywords in the text. It works well when the same words appear in the document."
      ],
      "metadata": {
        "id": "SMIgSbrlBHD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def failure_analysis():\n",
        "    q = \"quantum cryptography standards\"\n",
        "    print(\"Query:\", q)\n",
        "    print(\"\\nDense Results:\")\n",
        "    print(semantic_search(q)[\"text\"].iloc[0][:300])\n",
        "\n",
        "    print(\"\\nBM25 Results:\")\n",
        "    print(bm25_search(q)[\"text\"].iloc[0][:300])\n"
      ],
      "metadata": {
        "id": "HKRC86hLT-8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "failure_analysis()\n"
      ],
      "metadata": {
        "id": "ruSMkkXuUBKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure analysis is the systematic study of why a component, system, or process fails to perform its intended function. The main goal is to identify the root cause of failure and prevent it from happening again."
      ],
      "metadata": {
        "id": "EDMnmnWWAlbP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFw/IuaLY2VgJrjc6hjvj8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}